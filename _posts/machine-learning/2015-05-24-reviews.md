---
layout: page
title: Review Machine learning in action
categories: machine-learning
---
{% include JB/setup %}

使用回归模型，可以使用所有的数据来为模型服务！

如果只是找到最相近的几个数据点，然后来求他们的分类或者预测属性的话，那整个数据集中，只有几个点参与到的预测和分类中，其它的数据都几乎没有用到。

KNN的劣势：

1. 计算复杂度高，每次都要遍历整个数据集。
2. 只有最接近的K个数据参与到最后的分类或者预测中。

---

Linear classifiers

1. Score the weight of the each word (训练每个词的权重) 比如：
    + good: 1.0
    + beautiful: 1.3
    + awesome: 1.5
    + bad: -1.0
    + suck: -2.0
    + car: 0.0
    + house: 0.0
2. 把句子中的每个词的权重相加算出得分：
    + 大于0，positive
    + 小于0，negative

---

看来华盛顿大学的机器学习课程，才发现协同过滤真的很简单！这门课太棒了！

最简单的product-to-product推荐系统

- 首先他是一个`对角矩阵`
- 行和列一样的商品
- 这样的话，就能知道，两两商品同时出现的次数
- 这也就可以做协同过滤了

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 20 & 1  & 2  & 100 & 15 \\
\hline
深入浅出设计模式 & 1  & 30 & 16 & 110 & 3  \\
\hline
Effective Java   & 2  & 16 & 40 & 150 &  4  \\
\hline
三体             & 100 & 110 & 150 & 10000 & 250 \\
\hline
时间简史         & 15 & 3  & 4  & 250 & 70 \\
\hline
\end{array}
$$

##### Normalizing co-occurrence matrices 

Jaccard similarity，因为有些商品购买的特别多，无论买那个商品都会推荐它

who purchased `i and j` divided by who purchased `i or j`

$$
\frac{\text{# purchased } i \text{ and }  j }{\text{# purchased } i \text{ or }  j}
$$

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
深入浅出设计模式 & 0.05  & 1     & 0.53  & 0.27 & 0.10  \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
三体             & 0.05  & 0.27  & 0.27  & 1 & 0.25 \\
\hline
时间简史         & 0.75  & 0.10  & 0.10  & 0.25 & 1 \\
\hline
\end{array}
$$

如果用户同时购买了两件商品，或者更多的商品，可以想搜素引擎打分一样，给商品打分

1. 分别找出为这两个商品推荐的商品
2. 分别把这些权重相加，找出得分最高的商品


$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
Result           & 1.10  & 0.58  & 1.10  & 0.32 & 0.85 \\
\hline
\end{array}
$$

所以推荐的顺序是：时间简史、深入浅出审计模式、三体

---

#### Supervised Learning method

- KNN (k nearest neighbor)
    1. Calculate the distance will all the data.
    2. Find the nearest k data
    3. From this k nearest data find the most frequent category
- Decision Tree
    1. Find the most efficient propercity to divided the datas
    2. Check if all the sub tree has been divied
    3. If not recurive steps 1 and 2
- Naive Bayes
    - 根据朴素贝叶斯概率模型来计算一个目标可能出现在哪个分类中
    - 如果出现在分来A的概率大于出现在分类B的概率，则结果为A反之为B
- Regression
    - 找出或者画出一条可以拟合数据走向的线
    - 最小二乘
    - logistic regression
    - CART Classification-and-Regression-Trees
- SVM (support vector machine)
    - 找出支撑点
    - 利用kernel把地位的数据映射到高维求解
- Adaboost
    - 可以多次重复同一函数，也可用不同函数
    - 每次计算以后，增加错误数据的权重，减小正确数据的权重

#### Unsupervised Learning method

- K means
    - 首先定义K个中心店，然后计算所有点到这些点的距离
    - 把里这些点最近的点合成一簇，
    - 找出簇内的中心店，继续迭代
- Apriori
    - 原理是认为如果一项不频繁，那些包含它的集合也不是频繁项
- FP-Growth
    - 先构建FP tree
    - 扫描FP tree，获得频繁项

#### Other optimize

- PCA
    - 找出最能表现数据的维度作为坐标轴
    - 通过这种方式来降维
- SVD
    - 把一个大的系数矩阵，切分为几个矩阵来运算
    - 通过上面的方法来达到降维的目的
