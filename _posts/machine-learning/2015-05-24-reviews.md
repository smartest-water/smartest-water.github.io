---
layout: page
title: Review Machine Learning in Action
categories: machine-learning
---
{% include JB/setup %}

#### Machine Learning: Regression

- 使用回归模型，可以使用所有的数据来为模型服务！
- 如果只是找到最相近的几个数据点，然后来求他们的分类或者预测属性的话，那整个数据集中，只有几个点参与到的预测和分类中，其它的数据都几乎没有用到。
- 使用回归模型，不但可以用x，预测y；也可以用y，预测x

1. Simple linear regression
2. Multiple Regression
3. Assessing performance
    + Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.
4. Ridge Regression
5. Feature Selection & Lasso
6. Nearest Neighbors & Kernel Regression

---

#### 回归模型的advantage 和KNN的disadvantage

KNN的劣势：

1. 计算复杂度高，每次都要遍历整个数据集。
2. 只有最接近的K个数据参与到最后的分类或者预测中。

#### 在回归模型中，加入随着x轴的变化，周期变化的参数：

$$
y_i = w_0 + w_1t_i + w_2 \sin (2 \pi t_i / 12 - \Phi ) + \epsilon_i
$$

Equivalently

$$
y_i = w_0 + w_1t_i + w_2 \sin (2 \pi t_i / 12) + w_3 \cos (2 \pi t_i / 12) + \epsilon_i
$$

Generic basis expansion

$$
y_i = w_0h_0(x_i) + w_1h_1(x_i) + w_2 h_2(x_i)  + \cdots + w_D h_D(x_i)  + \epsilon_i \\

= \sum_{j=0}^D w_j h_j(x_i) + \epsilon_i
$$


#### 向量和矩阵 计算梯度

- Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:

$$
(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)^2
$$

- Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:

$$
2*(w[0]*[CONSTANT] + w[1]*[feature_1] + ... + w[i] *[feature_i] + ... + w[k]*[feature_k] - output)* [feature_i]
$$

- The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:

$$
2*error*[feature_i]
$$

- That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!
Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature[i] is just two times the dot product between the values of feature[i] and the current errors.

#### Assessing loss on the training set

- RMSE: root mean square error:

$$
\sqrt{ \frac{1}{N} \sum_{i=1}^N (y_i-f(x_i))^2 }
$$

- Generalization error vs model complexity
    + **判断结果是不是符合正态分布**
    + define overfitting
        1. training error less than other parameter
        2. test error larger than other parameter
    + split train and test data
        1. 留下足够的测试数据
        2. 然后使用剩下的所有数据做training

$$
\frac{1}{N \text{test}} \sum_{i=1}^N L(y_i-f(x_i))
$$

- 3 sources of error
    + Noise
    + Bias
    + Variance

- Bias-variance tradeoff
    + High-complexity has low bias
    + High-complexity has high variance 

- Bias of function estimator
    + 使用交叉验证
    + 在每个数据集上训练数据
    + 把每个数据集得到的结果求平均值

加入调节的参数


$$

$$



---

#### Linear classifiers

1. Score the weight of the each word (训练每个词的权重) 比如：
    + good: 1.0
    + beautiful: 1.3
    + awesome: 1.5
    + bad: -1.0
    + suck: -2.0
    + car: 0.0
    + house: 0.0
2. 把句子中的每个词的权重相加算出得分：
    + 大于0，positive
    + 小于0，negative

---

#### 基于商品的协同过滤

看来华盛顿大学的机器学习课程，才发现协同过滤真的很简单！这门课太棒了！

最简单的product-to-product推荐系统

- 首先他是一个`对角矩阵`
- 行和列一样的商品
- 这样的话，就能知道，两两商品同时出现的次数
- 这也就可以做协同过滤了

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 20 & 1  & 2  & 100 & 15 \\
\hline
深入浅出设计模式 & 1  & 30 & 16 & 110 & 3  \\
\hline
Effective Java   & 2  & 16 & 40 & 150 &  4  \\
\hline
三体             & 100 & 110 & 150 & 10000 & 250 \\
\hline
时间简史         & 15 & 3  & 4  & 250 & 70 \\
\hline
\end{array}
$$

##### Normalizing co-occurrence matrices

Jaccard similarity，因为有些商品购买的特别多，无论买那个商品都会推荐它

who purchased `i and j` divided by who purchased `i or j`

$$
\frac{\text{# purchased } i \text{ and }  j }{\text{# purchased } i \text{ or }  j}
$$

$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
深入浅出设计模式 & 0.05  & 1     & 0.53  & 0.27 & 0.10  \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
三体             & 0.05  & 0.27  & 0.27  & 1 & 0.25 \\
\hline
时间简史         & 0.75  & 0.10  & 0.10  & 0.25 & 1 \\
\hline
\end{array}
$$

如果用户同时购买了两件商品，或者更多的商品，可以想搜素引擎打分一样，给商品打分

1. 分别找出为这两个商品推荐的商品
2. 分别把这些权重相加，找出得分最高的商品


$$
\begin{array}{|c|c|c|c|c|c|c|} \hline X & 从一到无穷大 & 深入浅出设计模式 & Effective Java & 三体 & 时间简史 \\
\hline
从一到无穷大     & 1     & 0.05  & 0.10  & 0.05 & 0.75 \\
\hline
Effective Java   & 0.10  & 0.53  & 1     & 0.27 &0.10  \\
\hline
Result           & 1.10  & 0.58  & 1.10  & 0.32 & 0.85 \\
\hline
\end{array}
$$

所以推荐的顺序是：时间简史、深入浅出审计模式、三体

---

#### 基于内容的协同过滤

- user     有多个features
- movie  也有多个feature

- user = (f1,f2,f3,...,fn)
- movie = (k1,k2,k3,...,kn)

- user * movie = score(positve, negative)
- 用这种方法可以求出各个feature和weight
- 然后再用使用基于内容的协同过滤

##### matrix factorization

---

#### 评价系统的推荐系统

Recall

$$
\frac{\text{# liked & shown}}{\text{ # liked}}
$$

Precision

$$
\frac{\text{# liked & shown}}{\text{# shown}}
$$

Optimal recommenders

$$
Recall = 1 \\
Precision = 1
$$

Precision-recall curves

- We can use Area Under the Curve(AUC) method to evluate with curve is better.

<img width="40%" src="http://ivrl.epfl.ch/files/content/sites/ivrg/files/supplementary_material/RK_ICIP2010/images/MSSSvsOthers.jpg" alt="Precision-recall curves"/>

---

#### Word count document representation

##### Bag of words model

- Ignore order of words
- Count # of instances of each word in vocabulary

原来比较两个文章的相似度，比我想的还要简单


#### Supervised Learning method

- KNN (k nearest neighbor)
    1. Calculate the distance will all the data.
    2. Find the nearest k data
    3. From this k nearest data find the most frequent category
- Decision Tree
    1. Find the most efficient propercity to divided the datas
    2. Check if all the sub tree has been divied
    3. If not recurive steps 1 and 2
- Naive Bayes
    - 根据朴素贝叶斯概率模型来计算一个目标可能出现在哪个分类中
    - 如果出现在分来A的概率大于出现在分类B的概率，则结果为A反之为B
- Regression
    - 找出或者画出一条可以拟合数据走向的线
    - 最小二乘
    - logistic regression
    - CART Classification-and-Regression-Trees
- SVM (support vector machine)
    - 找出支撑点
    - 利用kernel把地位的数据映射到高维求解
- Adaboost
    - 可以多次重复同一函数，也可用不同函数
    - 每次计算以后，增加错误数据的权重，减小正确数据的权重

#### Unsupervised Learning method

- K means
    - 首先定义K个中心店，然后计算所有点到这些点的距离
    - 把里这些点最近的点合成一簇，
    - 找出簇内的中心店，继续迭代
- Apriori
    - 原理是认为如果一项不频繁，那些包含它的集合也不是频繁项
- FP-Growth
    - 先构建FP tree
    - 扫描FP tree，获得频繁项

#### Other optimize

- PCA
    - 找出最能表现数据的维度作为坐标轴
    - 通过这种方式来降维
- SVD
    - 把一个大的系数矩阵，切分为几个矩阵来运算
    - 通过上面的方法来达到降维的目的
