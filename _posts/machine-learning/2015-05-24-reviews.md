---
layout: page
title: Review Machine learning in action
categories: machine-learning
---
{% include JB/setup %}

---

使用回归模型，可以使用所有的数据来为模型服务！

如果只是找到最相近的几个数据点，然后来求他们的分类或者预测属性的话，那整个数据集中，只有几个点参与到的预测和分类中，其它的数据都几乎没有用到。

KNN的劣势：

1. 计算复杂度高，每次都要遍历整个数据集。
2. 只有最接近的K个数据参与到最后的分类或者预测中。

---

Linear classifiers

1. Score the weight of the each word (训练每个词的权重) 比如：
    + good: 1.0
    + beautiful: 1.3
    + awesome: 1.5
    + bad: -1.0
    + suck: -2.0
    + car: 0.0
    + house: 0.0
2. 把句子中的每个词的权重相加算出得分：
    + 大于0，positive
    + 小于0，negative

---

#### Supervised Learning method

- KNN (k nearest neighbor)
    1. Calculate the distance will all the data.
    2. Find the nearest k data
    3. From this k nearest data find the most frequent category
- Decision Tree
    1. Find the most efficient propercity to divided the datas
    2. Check if all the sub tree has been divied
    3. If not recurive steps 1 and 2
- Naive Bayes
    - 根据朴素贝叶斯概率模型来计算一个目标可能出现在哪个分类中
    - 如果出现在分来A的概率大于出现在分类B的概率，则结果为A反之为B
- Regression
    - 找出或者画出一条可以拟合数据走向的线
    - 最小二乘
    - logistic regression
    - CART Classification-and-Regression-Trees
- SVM (support vector machine)
    - 找出支撑点
    - 利用kernel把地位的数据映射到高维求解
- Adaboost
    - 可以多次重复同一函数，也可用不同函数
    - 每次计算以后，增加错误数据的权重，减小正确数据的权重

#### Unsupervised Learning method

- K means
    - 首先定义K个中心店，然后计算所有点到这些点的距离
    - 把里这些点最近的点合成一簇，
    - 找出簇内的中心店，继续迭代
- Apriori
    - 原理是认为如果一项不频繁，那些包含它的集合也不是频繁项
- FP-Growth
    - 先构建FP tree
    - 扫描FP tree，获得频繁项

#### Other optimize

- PCA
    - 找出最能表现数据的维度作为坐标轴
    - 通过这种方式来降维
- SVD
    - 把一个大的系数矩阵，切分为几个矩阵来运算
    - 通过上面的方法来达到降维的目的
